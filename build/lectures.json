[
	{
		"type": "lecture",
		"name": "TBA",
		"date": "TBA",
		"desc": "",
		"documents": [
		],
		"recordings": [
		]
	},

	{
		"type": "tutorial",
		"name": "Introduction to PyTorch + Working with the Lisa cluster",
		"date": "November 4, 2021 | 13.00-15.00 | Tutorial session",
		"desc": "This tutorial introduces the practical sessions, the TA organizer team, etc. Afterwards, we will discuss the PyTorch machine learning framework, and introduce you to the basic concepts of Tensors, computation graphs and GPU computation. We will continue with a small hands-on tutorial of building your own, first neural network in PyTorch.</p><p>We also provide a crash-course for working with the Lisa cluster, and how to setup your account for Lisa.</p>",
		"documents": [
			{"name": "Recording - Part 1 (Introduction)",
			 "link": "https://youtu.be/oluO8JiC7EA",
			 "type": "video"},
			{"name": "Recording - Part 2 (PyTorch Basics)",
			 "link": "https://youtu.be/wnKZZgFQY-E",
			 "type": "video"},
			{"name": "Recording - Part 3 (NNs in PyTorch)",
			 "link": "https://youtu.be/schbjeU5X2g",
			 "type": "video"},
			{"name": "Lisa Tutorial",
			 "link": "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial1/Lisa_Cluster.html",
			 "type": "notebook"},
			{"name": "PyTorch tutorial notebook",
			 "link": "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.html",
			 "type": "notebook"}
		]
	},

	{
		"type": "lecture",
		"name": "TBA",
		"date": "TBA",
		"desc": "",
		"documents": [
		],
		"recordings": [
		]
	},

	{
		"type": "lecture",
		"name": "TBA",
		"date": "TBA",
		"desc": "",
		"documents": [
		],
		"recordings": [
		]
	},

	{
		"type": "tutorial",
		"name": "Activation Functions",
		"date": "November 11, 2021 | 13.00-15.00 | Tutorial session + TA Q&A",
		"desc": "In this tutorial, we will discuss the role of activation functions in a neural network, and take a closer look at the optimization issues a poorly designed activation function can have.</p><p>After the presentation, there will by a TA session for Q&A for assignment 1, lecture content and more.",
		"documents": [
			{"name": "Recording - Part 1",
			 "link": "https://youtu.be/3C_m71NDkPs",
			 "type": "video"},
			{"name": "Recording - Part 2",
			 "link": "https://youtu.be/GjCfAeHGq9U",
			 "type": "video"},
			{"name": "Jupyter notebook",
			 "link": "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html",
			 "type": "notebook"}
		]
	},

	{
		"type": "lecture",
		"name": "TBA",
		"date": "TBA",
		"desc": "",
		"documents": [
		],
		"recordings": [
		]
	},

	{
		"type": "lecture",
		"name": "TBA",
		"date": "TBA",
		"desc": "",
		"documents": [
		],
		"recordings": [
		]
	},

	{
		"type": "tutorial",
		"name": "Optimization and Initialization",
		"date": "November 18, 2020 | 13.00-15.00 | Tutorial session + TA Q&A",
		"desc": "In this tutorial, we will discuss the importance of proper parameter initialization in deep neural networks, and how we can find a suitable one for our network. In addition, we will review the optimizers SGD and Adam, and compare them on complex loss surfaces.</p><p>After the presentation, there will by a TA session for Q&A for assignment 1, lecture content and more.",
		"documents": [
			{"name": "Recording - Part 1 (Initialization)",
			 "link": "https://youtu.be/X5m7bC4xCLY",
			 "type": "video"},
			{"name": "Recording - Part 2 (Optimization)",
			 "link": "https://youtu.be/UcRBZbAP9hM",
			 "type": "video"},
			{"name": "Jupyter notebook",
			 "link": "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial4/Optimization_and_Initialization.html",
			 "type": "notebook"}
		]
	},

	{
		"type": "lecture",
		"name": "TBA",
		"date": "TBA",
		"desc": "",
		"documents": [
		],
		"recordings": [
		]
	},

	{
		"type": "lecture",
		"name": "TBA",
		"date": "TBA",
		"desc": "",
		"documents": [
		],
		"recordings": [
		]
	},

	{
		"type": "tutorial",
		"name": "Inception, ResNet and DenseNet",
		"date": "November 25, 2020 | 13.00-15.00 | Tutorial session + TA Q&A",
		"desc": "In this tutorial, we will implement three popular, modern ConvNet architectures: GoogleNet, ResNet, and DenseNet. We will compare them on the CIFAR10 dataset, and discuss the advantages that made them popular and successful across many tasks.</p><p>After the presentation, there will by a TA session for Q&A for assignment 2, lecture content and more.",
		"documents": [
			{"name": "Recording - Part 1 (PyTorch Lightning)",
			 "link": "https://youtu.be/vjSSyGxlczs",
			 "type": "video"},
			{"name": "Recording - Part 2 (Inception and ResNet)",
			 "link": "https://youtu.be/9yRXqYJDHr4",
			 "type": "video"},
			{"name": "Recording - Part 3 (DenseNet and comparison)",
			 "link": "https://youtu.be/ELEqNwv9vkE",
			 "type": "video"},
			{"name": "Jupyter notebook",
			 "link": "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial5/Inception_ResNet_DenseNet.html",
			 "type": "notebook"}
		]
	},

	{
		"type": "lecture",
		"name": "TBA",
		"date": "TBA",
		"desc": "",
		"documents": [
		],
		"recordings": [
		]
	},

	{
		"type": "lecture",
		"name": "TBA",
		"date": "TBA",
		"desc": "",
		"documents": [
		],
		"recordings": [
		]
	},

	{
		"type": "tutorial",
		"name": "Transformers and Multi-Head Attention",
		"date": "December 2, 2020 | 13.00-15.00 | Tutorial session + TA Q&A",
		"desc": "In this tutorial, we will discuss the relatively new breakthrough architecture: Transformers. We will start from the basics of attention and multi-head attention, and build our own Transformer. We will perform experiments on sequence-to-sequence tasks and set anomaly detection.</p><p>After the presentation, there will by a TA session for Q&A for assignment 2, lecture content and more.",
		"documents": [
			{"name": "Recording - Part 1 (What is Attention + MH Attention)",
			 "link": "https://youtu.be/hGZ6wa07Vak",
			 "type": "video"},
			{"name": "Recording - Part 2 (Architecture and Training tricks)",
			 "link": "https://youtu.be/QdTgJ85E6YA",
			 "type": "video"},
			{"name": "Recording - Part 3 (Experiments)",
			 "link": "https://youtu.be/e7xvF2yS4Dg",
			 "type": "video"},
			{"name": "Jupyter notebook",
			 "link": "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html",
			 "type": "notebook"}
		]
	},

	{
		"type": "lecture",
		"name": "TBA",
		"date": "TBA",
		"desc": "",
		"documents": [
		],
		"recordings": [
		]
	},

	{
		"type": "lecture",
		"name": "TBA",
		"date": "TBA",
		"desc": "",
		"documents": [
		],
		"recordings": [
		]
	},

	{
		"type": "tutorial",
		"name": "Graph Neural Networks",
		"date": "December 9, 2020 | 13.00-15.00 | Tutorial session + TA Q&A",
		"desc": "In this tutorial, we will discuss the implementation of Graph Neural Networks. In the first part of the tutorial, we will implement the GCN and GAT layer ourselves. In the second part, we use PyTorch Geometric to look at node-level, edge-level and graph-level tasks.</p><p>After the presentation, there will by a TA session for Q&A for assignment 3, lecture content and more.",
		"documents": [
			{"name": "Recording - Part 1 (GCN and GAT)",
			 "link": "https://youtu.be/fK7d56Ly9q8",
			 "type": "video"},
			{"name": "Recording - Part 2 (Experiments)",
			 "link": "https://youtu.be/ZCNSUWe4a_Q",
			 "type": "video"},
			{"name": "Jupyter notebook",
			 "link": "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html",
			 "type": "notebook"}
		]
	},

	{
		"type": "lecture",
		"name": "TBA",
		"date": "TBA",
		"desc": "",
		"documents": [
		],
		"recordings": [
		]
	},

	{
		"type": "lecture",
		"name": "TBA",
		"date": "TBA",
		"desc": "",
		"documents": [
		],
		"recordings": [
		]
	},

	{
		"type": "tutorial",
		"name": "Deep Autoencoders",
		"date": "December 16, 2020 | 13.00-15.00 | Tutorial session + TA Q&A",
		"desc": "In this tutorial, we will discuss deep convolutional autoencoders and their applications. In the practical and lecture, you will see variational autoencoders (VAE), which add a stochastic part to vanilla autoencoders. Both have their advantages and applications, of which we visit image retrieval and compression for the vanilla auotoencoder.</p><p>After the presentation, there will by a TA session for Q&A for assignment 3, lecture content and more.",
		"documents": [
			{"name": "Recording - Part 1 (Building an AE)",
			 "link": "https://youtu.be/E2d8NRYt2e4",
			 "type": "video"},
			{"name": "Recording - Part 2 (Analysis)",
			 "link": "https://youtu.be/3UrX2mTY610",
			 "type": "video"},
			{"name": "Jupyter notebook",
			 "link": "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial9/AE_CIFAR10.html",
			 "type": "notebook"}
		]
	},

	{
		"type": "lecture",
		"name": "TBA",
		"date": "TBA",
		"desc": "",
		"documents": [
		],
		"recordings": [
		]
	}

]
